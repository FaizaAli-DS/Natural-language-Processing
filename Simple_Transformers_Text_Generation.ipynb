{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8xC7YYgJgNUgqzopDjQsy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaizaAli-DS/Natural-language-Processing/blob/all-about-AI-and-Data-Science/Simple_Transformers_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers_Text_Generation"
      ],
      "metadata": {
        "id": "KTRVqOiyWc3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVF7OMPtWSkb",
        "outputId": "8e15a663-e7eb-45ec-a0f3-06dba16e2fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline function is a high-level APIthe Hugging Face Transformers library that makes it easy to use pre-trained NLP models for various NLP tasks."
      ],
      "metadata": {
        "id": "gZInE4f5W5lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTKwtXM_Wkfa",
        "outputId": "612e4397-c252-48fa-8e4a-f59367ba919f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets creates a text generation pipeline by calling the pipeline function with the argument \"text-generation.\" This tells the library to load a pre-trained text generation model.\n",
        "\n",
        "The \"text-generation\" pipeline uses a model that has been pre-trained on a large corpus of text data and can generate text based on a given input or prompt."
      ],
      "metadata": {
        "id": "6VG5wWd0XXb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I am using Github to\"\n",
        "text_generator(prompt, max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GOjens5WkiW",
        "outputId": "5ffe7557-771c-46d4-bcdf-ca99033b8e2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'I am using Github to get around my \"Dependencies\"?\\n\\nYou can use the new git add \"deployment script\" with the'},\n",
              " {'generated_text': 'I am using Github to create this new list of people that are most likely to help this project and will be listed here.\\n\\nListed below'},\n",
              " {'generated_text': 'I am using Github to open various projects\\n\\nOpen up a script and save it to a folder. The script can be placed within a different folder'},\n",
              " {'generated_text': 'I am using Github to develop content in the open space, so I would like to get in touch with you when it is appropriate. Please use the'},\n",
              " {'generated_text': 'I am using Github to build my development machine. I found the project to be fairly simple, but did have a challenge for the project team, they'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I can use GPT2 for\"\n",
        "text_generator(prompt, max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpkZrUvMWklJ",
        "outputId": "bff0f5be-542d-4195-b6fe-afe794558ad0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"I can use GPT2 for the whole system, but I really don't want to use GPT7, so here's my solution.\\n\"},\n",
              " {'generated_text': \"I can use GPT2 for all these issues too, but we can't afford to buy these devices with lower price tags.\\n\\nIf your\"},\n",
              " {'generated_text': 'I can use GPT2 for this. In this case, what I want to do is to store the code in an MSPIRQL package'},\n",
              " {'generated_text': 'I can use GPT2 for Windows and Linux, and GPT3 for Mac users.\\n\\nThe code is in C++11 and will'},\n",
              " {'generated_text': 'I can use GPT2 for my own projects.\"\\n\\nThe first batch of files is named xor.txt and consists of two subdirect'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"AI based text generation is\"\n",
        "text_generator(prompt, max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1FP4ffGX_5I",
        "outputId": "b56e11d4-0b07-4e37-c2ec-96c86bf962e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'AI based text generation is more than just an improvement but is a powerful way to automate the work of AI.\\n\\nThere are already a handful of'},\n",
              " {'generated_text': 'AI based text generation is very popular nowadays. If you see similar problems on a blog post or something like that, go and help my team out.'},\n",
              " {'generated_text': 'AI based text generation is not required for the most common queries.\\n\\nIf you are looking for a simple example where the queries do not require queries'},\n",
              " {'generated_text': 'AI based text generation is the future. I love it when you can combine an open source project in your mind and put it here.\\n\\nWhat'},\n",
              " {'generated_text': \"AI based text generation is often not done before the target is reached. It's easy for us to have a little bit of experience in producing text using\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"AI based text generation was\"\n",
        "text_generator(prompt, max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO8SsRzjX_8Q",
        "outputId": "d8b36f45-ce63-4993-d4b0-4e5c2e56c95c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'AI based text generation was invented by the Chinese, which is an important step for the future of e-books, says Pritzker.\\n\\n'},\n",
              " {'generated_text': 'AI based text generation was a difficult one. For an example, consider how many students take GCSE calculus classes in New Zealand. Many say, \"'},\n",
              " {'generated_text': 'AI based text generation was the subject of research by former IBM research scientist Tim Anderson.\\n\\nThe two were not aware of each other on the day'},\n",
              " {'generated_text': 'AI based text generation was not able to achieve its intended goals, with limited input and input from partners. However, a series of research developments and experiments'},\n",
              " {'generated_text': 'AI based text generation was added to the browser when Firefox and Chrome entered the browsers of their respective browsers.\\n\\nThis was also implemented as a native'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}